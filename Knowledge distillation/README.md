#  Knoledge Distillation 
## Knoeldge Distillation의 이론은 간단하다. Teacher model(사전에 잘 학습이 된 모델)을 바탕으로 Student model(연산량이 낮은 모델)이 학습을 진행하는 것이다.
> 이로 얻을 수 있는 장점은 Limited environment에서의 연산량을 줄일 수 있으며, 파라메터의 사이즈도 훨씬 적기에 edge computing이 가능하다는 장점을 가진다. 
[Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)


```python

```
